# -*- coding: utf-8 -*-
"""Copy of train.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ehaCO_iZzvJ-hfKUBLGYl0DSQlhYRxm5
"""

from google.colab import drive
drive.mount('/content/grive')

import torch
from dataset import XYDataset
import sys
from utils import save_checkpoint, load_checkpoint
from torch.utils.data import DataLoader
import torch.nn as nn
import torch.optim as optim
import config
from tqdm import tqdm
from torchvision.utils import save_image
from discriminator_model import Discriminator
from generator_model import Generator

# X for input (initail/real) image, Y for output generated (fake) image

def train_fn(disc_X, disc_Y, gen_Y, gen_X, loader, opt_disc, opt_gen, l1, mse, d_scaler, g_scaler):
  loop = tqdm(loader, leave = True) # generate progress bar to track

  for idx, (Y, X) in enumerate(loop): #
    Y = Y.to(config.DEVICE)
    X = X.to(config.DEVICE)

    # Train Discriminators X and Y
    with torch.cuda.amp.autocast(): # needed for float 16
      fake_X = gen_X(Y) # generate fake image
      D_X_real = disc_X(X) # identify real image
      D_X_fake = disc_X(fake_X.detach()) # catergorize if generated fake image is close to real image
      D_X_real_loss = mse(D_X_real, torch.ones_like(D_X_real)) # real is one fake is zero
      D_X_fake_loss = mse(D_X_fake, torch.zeros_like(D_X_fake)) # real is one fake is zero
      D_X_loss = D_X_real_loss + D_X_fake_loss # sum of real and fake losses

      fake_Y = gen_Y(X) # generate fake image 
      D_Y_real = disc_Y(Y) # identify real image
      D_Y_fake = disc_Y(fake_Y.detach()) # catergorize if generated fake image is close to real image
      D_Y_real_loss = mse(D_Y_real, torch.ones_like(D_Y_real)) # real is one fake is zero
      D_Y_fake_loss = mse(D_Y_fake, torch.zeros_like(D_Y_fake)) # real is one fake is zero
      D_Y_loss = D_Y_real_loss + D_Y_fake_loss # sum of real and fake losses

      # Sum total discriminator losses
      D_loss = (D_X_loss + D_Y_loss)/2 # why did they divide by 2 in the paper? Removed division by 2 but results were worst

    opt_disc.zero_grad()
    d_scaler.scale(D_loss).backward()
    d_scaler.step(opt_disc)
    d_scaler.update()

    # Train Generators X and Y, Generators goal is to convince the discriminator into thinking the fake image is a real image
    with torch.cuda.amp.autocast():
      # Adversarial loss for both generators
      D_X_fake = disc_X(fake_X) # run fake X through the discriminator
      D_Y_fake = disc_Y(fake_Y) # run fake Y through the discriminator
      loss_G_X = mse(D_X_fake, torch.ones_like(D_X_fake)) # swap loss values for fake image to appear as real
      loss_G_Y = mse(D_Y_fake, torch.ones_like(D_Y_fake)) # swap loss values for fake image to appear as real, had an error in my code here

      # Cycle loss
      cycle_Y = gen_Y(fake_X) # reproduce original Y from a fake X image
      cycle_X = gen_X(fake_Y) # reproduce original X from a fake Y image
      cycle_Y_loss = l1(Y, cycle_Y) # cycle consistency loss for Y
      cycle_X_loss = l1(X, cycle_X) # cycle consistency loss for X

      # Identity loss, Decided to use for my dataset and project to see if reults improve
      identity_Y = gen_Y(Y)
      identity_X = gen_X(X)
      identity_Y_loss = l1(Y, identity_Y)
      identity_X_loss = l1(X, identity_X)

      # Sum together including lambda weight
      G_loss = (
          loss_G_Y # Adversarial loss for generator that generates a Y
          + loss_G_X # Adversarial loss for generator that generates a X
          + cycle_Y_loss * config.LAMBDA_CYCLE # Cycle Consistency loss for Y times cycle weight lambda
          + cycle_X_loss * config.LAMBDA_CYCLE # Cycle Consistency loss for X times cycle weight lambda
          + identity_X_loss * config.LAMBDA_IDENTITY # Decided to use for my dataset and project to see if results improve
          + identity_Y_loss * config.LAMBDA_IDENTITY # Decided to use for my dataset and project to see if results improve
      )

      opt_gen.zero_grad()
      g_scaler.scale(G_loss).backward()
      g_scaler.step(opt_gen)
      g_scaler.update()

      if idx % 10 == 0: # change "200" to output more images
        save_image(fake_X*0.5+0.5, f"saved_images/X_{idx}.png") # Inverse of normalization to correct coloring
        save_image(fake_Y*0.5+0.5, f"saved_images/Y_{idx}.png") # Inverse of normalization to correct coloring
    
def main():
  disc_X = Discriminator(in_channels = 3).to(config.DEVICE) # Initialize the discriminator X for classifying X image as real or fake
  disc_Y = Discriminator(in_channels = 3).to(config.DEVICE) # Initialize the discriminator Y for classifying Y image as real or fake
  gen_Y = Generator(img_channels = 3, num_residuals = 9).to(config.DEVICE) # Initialize the generator Y to take in a real image and tries to generate a fake image
  gen_X = Generator(img_channels = 3, num_residuals = 9).to(config.DEVICE) # Initialize the generator X to take in a real image and tries to generate a fake image
  opt_disc = optim.Adam( # Initialize the combined discriminator optimizer
      list(disc_X.parameters()) + list(disc_Y.parameters()),
      lr = config.LEARNING_RATE,
      betas = (0.5, 0.999), # paper's code specifies 0.5 for the momentum term and 0.999 for beta two
  )
  opt_gen = optim.Adam( # Initialize the combined generator optimizer
      list(gen_Y.parameters()) + list(gen_X.parameters()),
      lr = config.LEARNING_RATE,
      betas = (0.5, 0.999), # paper's code specifies 0.5 for the momentum term and 0.999 for beta two
  )

  L1 = nn.L1Loss() # used for cycle consistency loss and identity loss
  mse = nn.MSELoss() # mean squared error loss used for adversarial loss

  if config.LOAD_MODEL:
    load_checkpoint(
        config.CHECKPOINT_GEN_X, gen_X, opt_gen, config.LEARNING_RATE
    )
    load_checkpoint(
        config.CHECKPOINT_GEN_Y, gen_Y, opt_gen, config.LEARNING_RATE
    )
    load_checkpoint(
        config.CHECKPOINT_DISCRIMINATOR_X, disc_X, opt_disc, config.LEARNING_RATE
    )
    load_checkpoint(
        config.CHECKPOINT_DISCRIMINATOR_Y, disc_Y, opt_disc, config.LEARNING_RATE
    )

  dataset = XYDataset(
      root_X = config.TRAIN_DIR+"/X", root_Y = config.TRAIN_DIR+"/Y", transform = config.transforms # Change "/X" and "/Y" as needed 
  )
  loader = DataLoader(
      dataset,
      batch_size = config.BATCH_SIZE,
      shuffle = True,
      num_workers = config.NUM_WORKERS,
      pin_memory = True
  )
  g_scaler = torch.cuda.amp.GradScaler()
  d_scaler = torch.cuda.amp.GradScaler()
  
  for epoch in range(config.NUM_EPOCHS):
    train_fn(disc_X, disc_Y, gen_Y, gen_X, loader, opt_disc, opt_gen, L1, mse, d_scaler, g_scaler)

    if config.SAVE_MODEL:
      save_checkpoint(gen_X, opt_gen, filename = config.CHECKPOINT_GEN_X)
      save_checkpoint(gen_Y, opt_gen, filename = config.CHECKPOINT_GEN_Y)
      save_checkpoint(disc_X, opt_disc, filename = config.CHECKPOINT_DISCRIMINATOR_X)
      save_checkpoint(disc_Y, opt_disc, filename = config.CHECKPOINT_DISCRIMINATOR_Y)

if __name__ == "__main__":
  main()